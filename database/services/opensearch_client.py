# OpenSearchClient: ì‹¤ì „ ì„œë¹„ìŠ¤ìš© OpenSearch ì—°ë™ í´ë˜ìŠ¤
from opensearchpy import OpenSearch, exceptions, helpers, ConnectionTimeout
from sentence_transformers import SentenceTransformer
from FlagEmbedding import FlagReranker
import logging
import re
import json
import time
from typing import List, Dict, Any, Optional, Union
from config import settings

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

class OpenSearchClient:
    def __init__(self, max_retries: int = 3, timeout: int = 10):
        """OpenSearch í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.client = None
        self._model = None
        self._reranker = None
        self._embedding_dim = None
        self._default_embedding_dim = 1024
        
        # í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        self.client = self._create_client_with_retry(max_retries, timeout)
        
        # ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ)
        if self.client:
            self._preload_models()

    def _preload_models(self):
        """ëª¨ë¸ë“¤ì„ ë¯¸ë¦¬ ë¡œë“œí•©ë‹ˆë‹¤ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ)"""
        try:
            logger.info("ğŸš€ ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì‹œì‘...")
            
            # ì„ë² ë”© ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ
            if not self._model:
                logger.info("ğŸ¤– ì„ë² ë”© ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì¤‘...")
                self._model = self._embeddings_model()
                if self._model:
                    self._embedding_dim = len(self._model.encode("dummy_text"))
                    logger.info(f"âœ… ì„ë² ë”© ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì™„ë£Œ (ì°¨ì›: {self._embedding_dim})")
                else:
                    logger.error("âŒ ì„ë² ë”© ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì‹¤íŒ¨")
            
            # ì¬ìˆœìœ„ ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ
            if not self._reranker:
                logger.info("ğŸ”„ ì¬ìˆœìœ„ ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì¤‘...")
                self._reranker = self._rerank_model()
                if self._reranker:
                    logger.info("âœ… ì¬ìˆœìœ„ ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì™„ë£Œ")
                else:
                    logger.error("âŒ ì¬ìˆœìœ„ ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì‹¤íŒ¨")
                    
            logger.info("ğŸ‰ ëª¨ë“  ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")

    def _create_client_with_retry(self, max_retries: int, timeout: int):
        """
        ì¬ì‹œë„ ë¡œì§ì„ í¬í•¨í•œ OpenSearch í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        
        Args:
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            timeout: ì—°ê²° íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            
        Returns:
            OpenSearch í´ë¼ì´ì–¸íŠ¸ ë˜ëŠ” None
        """
        try:
            # ì¤‘ì•™í™”ëœ ì„¤ì •ì—ì„œ OpenSearch ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            opensearch_config = settings.get_opensearch_config()
            host = opensearch_config["host"]
            port = opensearch_config["port"]
            username = opensearch_config.get("username", "admin")
            password = opensearch_config["password"]
            
            logger.info(f"ğŸ”— OpenSearch ì—°ê²° ì‹œë„: {host}:{port}, ì‚¬ìš©ì: {username}")
            
            for attempt in range(max_retries):
                try:
                    logger.info(f"ğŸ“¡ ì—°ê²° ì‹œë„ {attempt + 1}/{max_retries} (íƒ€ì„ì•„ì›ƒ: {timeout}ì´ˆ)")
                    
                    # OpenSearch í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
                    client = OpenSearch(
                        hosts=[{"host": host, "port": port}],
                        timeout=timeout,
                        verify_certs=False,  # ê°œë°œ í™˜ê²½ì—ì„œëŠ” SSL ê²€ì¦ ë¹„í™œì„±í™”
                        ssl_show_warn=False
                    )
                    
                    # ping í…ŒìŠ¤íŠ¸ë¡œ ì—°ê²° í™•ì¸
                    if client.ping():
                        logger.info(f"âœ… OpenSearch ì—°ê²° ì„±ê³µ (ì‹œë„ {attempt + 1}/{max_retries})")
                        return client
                    else:
                        logger.warning(f"âš ï¸ OpenSearch ping ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries})")
                        
                except ConnectionTimeout as e:
                    logger.warning(f"â° ì—°ê²° íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                except Exception as e:
                    logger.warning(f"âŒ OpenSearch ì—°ê²° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                    logger.warning(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
                
                # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 2  # 2ì´ˆ, 4ì´ˆ, 6ì´ˆ...
                    logger.info(f"â³ {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)
            
            logger.error(f"âŒ OpenSearch ì—°ê²° ìµœì¢… ì‹¤íŒ¨ ({max_retries}íšŒ ì‹œë„)")
            return None
            
        except Exception as e:
            logger.error(f"âŒ OpenSearch ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    @property
    def model(self):
        """ì„ë² ë”© ëª¨ë¸ ì§€ì—° ë¡œë”©"""
        if self._model is None:
            logger.info("ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            self._model = self._embeddings_model()
            if self._model:
                self._embedding_dim = len(self._model.encode("dummy_text"))
                logger.info(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ì°¨ì›: {self._embedding_dim})")
            else:
                logger.error("âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
        return self._model

    @property
    def reranker(self):
        """ì¬ìˆœìœ„ ëª¨ë¸ ì§€ì—° ë¡œë”©"""
        if self._reranker is None:
            logger.info("ğŸ”„ ì¬ìˆœìœ„ ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            self._reranker = self._rerank_model()
            if self._reranker:
                logger.info("âœ… ì¬ìˆœìœ„ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            else:
                logger.error("âŒ ì¬ìˆœìœ„ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
        return self._reranker

    @property
    def embedding_dim(self):
        """ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        if self._embedding_dim is None:
            # KURE-v1 ëª¨ë¸ì€ 1024ì°¨ì› ë²¡í„°ë¥¼ ìƒì„±
            return 1024
        return self._embedding_dim

    def _check_client(self) -> bool:
        """í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
        if not self.client:
            logger.error("OpenSearch í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        return True

    def _format_search_results(self, hits: List[Dict], source_type: str) -> List[Dict[str, Any]]:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
        
        Args:
            hits: OpenSearch ê²€ìƒ‰ ê²°ê³¼ hits
            source_type: ê²€ìƒ‰ ì†ŒìŠ¤ íƒ€ì… (ì˜ˆ: "opensearch_vector_search")
            
        Returns:
            í¬ë§·íŒ…ëœ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        for hit in hits:
            source = hit["_source"]
            results.append({
                "content": source.get("content", ""),
                "metadata": {
                    "document_id": source.get("document_id"),
                    "chunk_index": source.get("chunk_index"),
                    "file_name": source.get("file_name"),
                    "title": source.get("title")
                },
                "score": hit["_score"],
                "rank": len(results) + 1,
                "source": source_type
            })
        return results

    def chunk_text_to_sentences(self, text: str, document_type: str = "report") -> List[Dict[str, Any]]:
        """
        ë¬¸ì„œ ì¢…ë¥˜ì— ë”°ë¼ í…ìŠ¤íŠ¸ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
        
        Args:
            text: ë¶„í• í•  í…ìŠ¤íŠ¸
            document_type: ë¬¸ì„œ ì¢…ë¥˜ ("regulation" ë˜ëŠ” "report")
            
        Returns:
            ì²­í‚¹ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ê° ì²­í¬ëŠ” metadata í¬í•¨)
        """
        if document_type == "regulation":
            return self._chunk_regulation_document(text)
        else:
            return self._chunk_report_document(text)

    def _chunk_regulation_document(self, text: str) -> List[Dict[str, Any]]:
        """
        ë‚´ë¶€ ê·œì • ë¬¸ì„œë¥¼ ì¥/ì¡° ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
        
        Args:
            text: ë‚´ë¶€ ê·œì • ë¬¸ì„œ í…ìŠ¤íŠ¸
            
        Returns:
            ì²­í‚¹ëœ ê·œì • ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        chunks = []
        
        # ì¥(Chapter) íŒ¨í„´ ë§¤ì¹­
        chapter_pattern = r'ì œ(\d+)ì¥\s*([^\n]+)'
        article_pattern = r'ì œ(\d+)ì¡°\s*\[([^\]]+)\]\s*([^\n]+)'
        
        lines = text.split('\n')
        current_chapter = None
        current_article = None
        current_content = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # ì¥(Chapter) ë§¤ì¹­
            chapter_match = re.match(chapter_pattern, line)
            if chapter_match:
                # ì´ì „ ì²­í¬ ì €ì¥
                if current_content and current_chapter:
                    chunks.append({
                        "content": '\n'.join(current_content),
                        "chapter": current_chapter,
                        "article": current_article,
                        "metadata": {
                            "type": "regulation",
                            "chapter_num": current_chapter.get("number"),
                            "chapter_title": current_chapter.get("title"),
                            "article_num": current_article.get("number") if current_article else None,
                            "article_title": current_article.get("title") if current_article else None
                        }
                    })
                
                # ìƒˆ ì¥ ì‹œì‘
                current_chapter = {
                    "number": chapter_match.group(1),
                    "title": chapter_match.group(2).strip()
                }
                current_article = None
                current_content = [line]
                continue
            
            # ì¡°(Article) ë§¤ì¹­
            article_match = re.match(article_pattern, line)
            if article_match:
                # ì´ì „ ì²­í¬ ì €ì¥
                if current_content and current_chapter:
                    chunks.append({
                        "content": '\n'.join(current_content),
                        "chapter": current_chapter,
                        "article": current_article,
                        "metadata": {
                            "type": "regulation",
                            "chapter_num": current_chapter.get("number"),
                            "chapter_title": current_chapter.get("title"),
                            "article_num": current_article.get("number") if current_article else None,
                            "article_title": current_article.get("title") if current_article else None
                        }
                    })
                
                # ìƒˆ ì¡° ì‹œì‘
                current_article = {
                    "number": article_match.group(1),
                    "title": article_match.group(2).strip()
                }
                current_content = [line]
                continue
            
            # ì¼ë°˜ ë‚´ìš© ì¶”ê°€
            current_content.append(line)
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_content and current_chapter:
            chunks.append({
                "content": '\n'.join(current_content),
                "chapter": current_chapter,
                "article": current_article,
                "metadata": {
                    "type": "regulation",
                    "chapter_num": current_chapter.get("number"),
                    "chapter_title": current_chapter.get("title"),
                    "article_num": current_article.get("number") if current_article else None,
                    "article_title": current_article.get("title") if current_article else None
                }
            })
        
        # ë¹ˆ ì²­í¬ ì œê±° ë° ìµœì†Œ ê¸¸ì´ í•„í„°ë§
        filtered_chunks = []
        for chunk in chunks:
            content = chunk["content"].strip()
            if len(content) > 10:  # ìµœì†Œ 10ì ì´ìƒ
                filtered_chunks.append(chunk)
        
        logger.info(f"ë‚´ë¶€ ê·œì • ë¬¸ì„œ ì²­í‚¹ ì™„ë£Œ: {len(filtered_chunks)}ê°œ ì²­í¬ ìƒì„±")
        return filtered_chunks

    def _chunk_report_document(self, text: str) -> List[Dict[str, Any]]:
        """
        ë³´ê³ ì„œ ë¬¸ì„œë¥¼ ì†Œì œëª© ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
        
        Args:
            text: ë³´ê³ ì„œ ë¬¸ì„œ í…ìŠ¤íŠ¸
            
        Returns:
            ì²­í‚¹ëœ ë³´ê³ ì„œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        chunks = []
        
        # ì†Œì œëª© íŒ¨í„´ë“¤ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
        subtitle_patterns = [
            r'^(\d+\.\s*[^\n]+)',  # 1. ì œëª©
            r'^([A-Z]\.\s*[^\n]+)',  # A. ì œëª©
            r'^([ê°€-í£]+\.\s*[^\n]+)',  # ê°€. ì œëª©
            r'^([^\n]+)\n[-=]{3,}',  # ì œëª©\n--- ë˜ëŠ” ===
            r'^##\s*([^\n]+)',  # ## ì œëª©
            r'^#\s*([^\n]+)',  # # ì œëª©
        ]
        
        lines = text.split('\n')
        current_subtitle = None
        current_content = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # ì†Œì œëª© ë§¤ì¹­ í™•ì¸
            subtitle_found = False
            for pattern in subtitle_patterns:
                match = re.match(pattern, line)
                if match:
                    # ì´ì „ ì²­í¬ ì €ì¥
                    if current_content and current_subtitle:
                        chunks.append({
                            "content": '\n'.join(current_content),
                            "subtitle": current_subtitle,
                            "metadata": {
                                "type": "report",
                                "subtitle": current_subtitle,
                                "subtitle_level": self._get_subtitle_level(current_subtitle)
                            }
                        })
                    
                    # ìƒˆ ì†Œì œëª© ì‹œì‘
                    current_subtitle = match.group(1) if len(match.groups()) > 0 else line
                    current_content = [line]
                    subtitle_found = True
                    break
            
            if not subtitle_found:
                # ì¼ë°˜ ë‚´ìš© ì¶”ê°€
                current_content.append(line)
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_content and current_subtitle:
            chunks.append({
                "content": '\n'.join(current_content),
                "subtitle": current_subtitle,
                "metadata": {
                    "type": "report",
                    "subtitle": current_subtitle,
                    "subtitle_level": self._get_subtitle_level(current_subtitle)
                }
            })
        
        # ì†Œì œëª©ì´ ì—†ëŠ” ê²½ìš° ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        if not chunks:
            sentences = re.split(r'[.!?]+', text)
            for i, sentence in enumerate(sentences):
                sentence = sentence.strip()
                if len(sentence) > 20:  # ìµœì†Œ 20ì ì´ìƒ
                    chunks.append({
                        "content": sentence,
                        "subtitle": f"ë¬¸ì¥ {i+1}",
                        "metadata": {
                            "type": "report",
                            "subtitle": f"ë¬¸ì¥ {i+1}",
                            "subtitle_level": 0
                        }
                    })
        
        # ë¹ˆ ì²­í¬ ì œê±° ë° ìµœì†Œ ê¸¸ì´ í•„í„°ë§
        filtered_chunks = []
        for chunk in chunks:
            content = chunk["content"].strip()
            if len(content) > 20:  # ìµœì†Œ 20ì ì´ìƒ
                filtered_chunks.append(chunk)
        
        logger.info(f"ë³´ê³ ì„œ ë¬¸ì„œ ì²­í‚¹ ì™„ë£Œ: {len(filtered_chunks)}ê°œ ì²­í¬ ìƒì„±")
        return filtered_chunks

    def _get_subtitle_level(self, subtitle: str) -> int:
        """
        ì†Œì œëª©ì˜ ë ˆë²¨ì„ íŒë‹¨í•©ë‹ˆë‹¤.
        
        Args:
            subtitle: ì†Œì œëª© í…ìŠ¤íŠ¸
            
        Returns:
            ì†Œì œëª© ë ˆë²¨ (1, 2, 3, ...)
        """
        if re.match(r'^\d+\.', subtitle):
            return 1
        elif re.match(r'^[A-Z]\.', subtitle):
            return 2
        elif re.match(r'^[ê°€-í£]\.', subtitle):
            return 3
        elif subtitle.startswith('##'):
            return 2
        elif subtitle.startswith('#'):
            return 1
        else:
            return 1

    def _rerank_model(self):
        """ì¬ìˆœìœ„ ëª¨ë¸ ë¡œë“œ (private ë©”ì„œë“œ)"""
        try:
            logger.info("ğŸ”„ BGE Reranker ëª¨ë¸ ë¡œë“œ ì¤‘...")
            reranker = FlagReranker('dragonkue/bge-reranker-v2-m3-ko', use_fp16=True, use_auth_token=None)
            logger.info("âœ… BGE Reranker ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return reranker
        except Exception as e:
            logger.error(f"âŒ Reranker ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def _embeddings_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (private ë©”ì„œë“œ)"""
        try:
            logger.info("ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
            model = SentenceTransformer("dragonkue/snowflake-arctic-embed-l-v2.0-ko")
            logger.info("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return model
        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    @property
    def DOCUMENT_INDEX_MAPPING(self) -> Dict[str, Any]:
        return {
            "settings": {
                "index": {
                    "knn": True
                }
            },
            "mappings": {
                "properties": {
                    "document_id": {"type": "keyword"},
                    "chunk_index": {"type": "integer"},
                    "content": {"type": "text"},
                    "embedding": {
                        "type": "knn_vector",
                        "dimension": self.embedding_dim,
                        "method": {
                            "name": "hnsw",
                            "space_type": "cosinesimil",
                            "engine": "lucene"
                        }
                    },
                    "file_name": {"type": "keyword"},
                    "title": {"type": "text"},
                    "document_type": {"type": "keyword"},
                    # ë‚´ë¶€ ê·œì • ê´€ë ¨ í•„ë“œ
                    "chapter_num": {"type": "keyword"},
                    "chapter_title": {"type": "text"},
                    "article_num": {"type": "keyword"},
                    "article_title": {"type": "text"},
                    # ë³´ê³ ì„œ ê´€ë ¨ í•„ë“œ
                    "subtitle": {"type": "text"},
                    "subtitle_level": {"type": "integer"}
                }
            }
        }

    def create_index(self, index_name: str, mapping: Optional[Dict[str, Any]] = None) -> bool:
        """
        ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            index_name: ìƒì„±í•  ì¸ë±ìŠ¤ ì´ë¦„
            mapping: ì¸ë±ìŠ¤ ë§¤í•‘ (Noneì´ë©´ ê¸°ë³¸ ë§¤í•‘ ì‚¬ìš©)
            
        Returns:
            ì¸ë±ìŠ¤ ìƒì„± ì„±ê³µ ì—¬ë¶€
        """
        if not self._check_client():
            return False
        
        try:
            if not self.client.indices.exists(index=index_name):
                mapping_to_use = mapping or self.DOCUMENT_INDEX_MAPPING
                self.client.indices.create(index=index_name, body=mapping_to_use)
                logger.info(f"'{index_name}' ì¸ë±ìŠ¤ë¥¼ ë§¤í•‘ê³¼ í•¨ê»˜ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
                return True
            logger.info(f"'{index_name}' ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
            return True
        except exceptions.OpenSearchException as e:
            logger.error(f"ì¸ë±ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False

    def create_index_with_mapping(self, index_name: str, mapping: Dict[str, Any]) -> bool:
        """ì‚¬ìš©ì ì •ì˜ ë§¤í•‘ìœ¼ë¡œ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        return self.create_index(index_name, mapping)

    def create_index_if_not_exists(self, index_name: str) -> bool:
        """ê¸°ë³¸ ë§¤í•‘ìœ¼ë¡œ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        return self.create_index(index_name)

    def index_document(self, index_name: str, document: Dict[str, Any], refresh: bool = False) -> Optional[Dict[str, Any]]:
        if not self._check_client():
            return None
        try:
            params = {"refresh": "true" if refresh else "false"}
            response = self.client.index(index=index_name, body=document, params=params)
            logger.info(f"'{index_name}' ì¸ë±ìŠ¤ì— ë¬¸ì„œ ID '{response['_id']}'ë¡œ ìƒ‰ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return response
        except exceptions.RequestError as e:
            logger.error(f"ë¬¸ì„œ ìƒ‰ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì˜ëª»ëœ ìš”ì²­): {e}")
        except exceptions.OpenSearchException as e:
            logger.error(f"ë¬¸ì„œ ìƒ‰ì¸ ì¤‘ ì˜ˆì™¸ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

    def bulk_index_documents(self, index_name: str, documents: List[Dict[str, Any]], refresh: bool = False) -> bool:
        if not self._check_client():
            return False
        actions = [
            {"_index": index_name, "_source": doc}
            for doc in documents
        ]
        try:
            success, failed = helpers.bulk(self.client, actions, refresh=refresh)
            logger.info(f"Bulk ì‘ì—… ì™„ë£Œ: ì„±ê³µ {success}ê±´, ì‹¤íŒ¨ {len(failed)}ê±´")
            return not failed
        except exceptions.OpenSearchException as e:
            logger.error(f"Bulk ìƒ‰ì¸ ì¤‘ ì˜ˆì™¸ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False

    def search_document(self, index_name: str, query: Dict[str, Any]) -> List[Dict[str, Any]]:
        if not self._check_client():
            return []
        try:
            response = self.client.search(index=index_name, body=query)
            hits = response["hits"]["hits"]
            logger.info(f"'{index_name}' ì¸ë±ìŠ¤ì—ì„œ {len(hits)}ê°œì˜ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return [{"score": hit["_score"], "source": hit["_source"]} for hit in hits]
        except exceptions.NotFoundError:
            logger.warning(f"ê²€ìƒ‰ ì‹¤íŒ¨: '{index_name}' ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        except exceptions.RequestError as e:
            logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì˜ëª»ëœ ì¿¼ë¦¬): {e}")
        except exceptions.OpenSearchException as e:
            logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜ˆì™¸ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

    def index_document_chunks(self, index_name: str, doc_id: int, doc_title: str, file_name: str, text: str, document_type: str = "report") -> bool:
        """
        ë¬¸ì„œë¥¼ ì²­í‚¹í•˜ì—¬ OpenSearchì— ì¸ë±ì‹±í•©ë‹ˆë‹¤.
        
        Args:
            index_name: ì¸ë±ìŠ¤ ì´ë¦„
            doc_id: ë¬¸ì„œ ID
            doc_title: ë¬¸ì„œ ì œëª©
            file_name: íŒŒì¼ëª…
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            document_type: ë¬¸ì„œ ì¢…ë¥˜ ("regulation" ë˜ëŠ” "report")
            
        Returns:
            ì¸ë±ì‹± ì„±ê³µ ì—¬ë¶€
        """
        if not self._check_client():
            return False
        
        try:
            # ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒì„±
            self.create_index_if_not_exists(index_name)
            
            # ë¬¸ì„œ ì²­í‚¹ ìˆ˜í–‰
            chunks = self.chunk_text_to_sentences(text, document_type)
            
            if not chunks:
                logger.warning(f"ë¬¸ì„œ {doc_id}ì—ì„œ ì²­í‚¹í•  ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # ì²­í‚¹ëœ ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì¸ë±ì‹±
            documents = []
            for i, chunk in enumerate(chunks):
                content = chunk["content"]
                if content.strip():  # ë¹ˆ ë‚´ìš© ì œì™¸
                    # ë‚´ìš©ì„ ë²¡í„°ë¡œ ë³€í™˜
                    embedding = self.model.encode(content)
                    
                    # ë¬¸ì„œ ì •ë³´ êµ¬ì„±
                    document = {
                        "document_id": doc_id,
                        "chunk_index": i,
                        "content": content,
                        "embedding": embedding.tolist(),
                        "file_name": file_name,
                        "title": doc_title,
                        "document_type": document_type
                    }
                    
                    # ë¬¸ì„œ ì¢…ë¥˜ë³„ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    if document_type == "regulation":
                        metadata = chunk.get("metadata", {})
                        document.update({
                            "chapter_num": metadata.get("chapter_num"),
                            "chapter_title": metadata.get("chapter_title"),
                            "article_num": metadata.get("article_num"),
                            "article_title": metadata.get("article_title")
                        })
                    else:  # report
                        metadata = chunk.get("metadata", {})
                        document.update({
                            "subtitle": metadata.get("subtitle"),
                            "subtitle_level": metadata.get("subtitle_level", 1)
                        })
                    
                    documents.append(document)
            
            # Bulk ì¸ë±ì‹± ìˆ˜í–‰
            success = self.bulk_index_documents(index_name, documents, refresh=True)
            
            if success:
                logger.info(f"ë¬¸ì„œ {doc_id}ì˜ {len(documents)}ê°œ ì²­í‚¹ì„ OpenSearchì— ì €ì¥í–ˆìŠµë‹ˆë‹¤. (ë¬¸ì„œ íƒ€ì…: {document_type})")
            else:
                logger.error(f"ë¬¸ì„œ {doc_id}ì˜ ì²­í‚¹ ì¸ë±ì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
            return success
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì²­í‚¹ ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
            return False

    def delete_document_chunks(self, index_name: str, document_id: int) -> bool:
        """
        íŠ¹ì • ë¬¸ì„œì˜ ëª¨ë“  ì²­í‚¹ì„ ì‚­ì œí•©ë‹ˆë‹¤.
        
        Args:
            index_name: ì¸ë±ìŠ¤ ì´ë¦„
            document_id: ì‚­ì œí•  ë¬¸ì„œ ID
            
        Returns:
            ì‚­ì œ ì„±ê³µ ì—¬ë¶€
        """
        if not self._check_client():
            return False
        
        try:
            # ë¬¸ì„œ IDë¡œ ëª¨ë“  ì²­í‚¹ ì‚­ì œ
            query = {
                "query": {
                    "term": {
                        "document_id": document_id
                    }
                }
            }
            
            response = self.client.delete_by_query(index=index_name, body=query)
            deleted_count = response.get("deleted", 0)
            
            logger.info(f"ë¬¸ì„œ {document_id}ì˜ {deleted_count}ê°œ ì²­í‚¹ì„ OpenSearchì—ì„œ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
            return True
            
        except exceptions.OpenSearchException as e:
            logger.error(f"ë¬¸ì„œ ì²­í‚¹ ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False

    def create_search_pipeline(self, pipeline_id: str = "hybrid-minmax-pipeline") -> bool:
        """
        OpenSearch 3.0+ í˜¸í™˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìš© search pipeline ìƒì„±
        
        Args:
            pipeline_id: ìƒì„±í•  íŒŒì´í”„ë¼ì¸ ID
            
        Returns:
            íŒŒì´í”„ë¼ì¸ ìƒì„± ì„±ê³µ ì—¬ë¶€
        """
        if not self._check_client():
            return False
            
        pipeline_body = {
            "description": "í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ì •ê·œí™” ë° ê²°í•© íŒŒì´í”„ë¼ì¸",
            "phase_results_processors": [
                {
                    "normalization-processor": {
                        "normalization": { 
                            "technique": "min_max" 
                        },
                        "combination": {
                            "technique": "arithmetic_mean",
                            "parameters": {
                                "weights": [0.3, 0.7]  # BM25: 0.3, ë²¡í„°: 0.7
                            }
                        }
                    }
                }
            ]
        }

        try:
            # íŒŒì´í”„ë¼ì¸ ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸
            response = self.client.transport.perform_request(
                method="PUT",
                url=f"/_search/pipeline/{pipeline_id}",
                body=pipeline_body
            )
            logger.info(f"âœ… Search pipeline '{pipeline_id}' ìƒì„±/ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            logger.debug(f"ì‘ë‹µ: {response}")
            return True
        except Exception as e:
            logger.error(f"âŒ Search pipeline ìƒì„± ì‹¤íŒ¨: {e}")
            return False

    def get_search_pipeline(self, pipeline_id: str = "hybrid-minmax-pipeline") -> Optional[Dict[str, Any]]:
        """
        ìƒì„±ëœ search pipeline ì •ë³´ ì¡°íšŒ
        
        Args:
            pipeline_id: ì¡°íšŒí•  íŒŒì´í”„ë¼ì¸ ID
            
        Returns:
            íŒŒì´í”„ë¼ì¸ ì •ë³´ ë˜ëŠ” None
        """
        if not self._check_client():
            return None
            
        try:
            response = self.client.transport.perform_request(
                method="GET",
                url=f"/_search/pipeline/{pipeline_id}"
            )
            logger.info(f"ğŸ“‹ Search pipeline '{pipeline_id}' ì •ë³´ ì¡°íšŒ ì™„ë£Œ")
            return response
        except Exception as e:
            logger.error(f"âŒ Search pipeline ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None

    def delete_search_pipeline(self, pipeline_id: str = "hybrid-minmax-pipeline") -> bool:
        """
        search pipeline ì‚­ì œ
        
        Args:
            pipeline_id: ì‚­ì œí•  íŒŒì´í”„ë¼ì¸ ID
            
        Returns:
            ì‚­ì œ ì„±ê³µ ì—¬ë¶€
        """
        if not self._check_client():
            return False
            
        try:
            response = self.client.transport.perform_request(
                method="DELETE",
                url=f"/_search/pipeline/{pipeline_id}"
            )
            logger.info(f"ğŸ—‘ï¸ Search pipeline '{pipeline_id}' ì‚­ì œ ì™„ë£Œ")
            return True
        except Exception as e:
            logger.error(f"âŒ Search pipeline ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False

    def search_with_pipeline(self, 
                           query_text: str,
                           keywords: Union[str, List[str]] = None, 
                           pipeline_id: str = "hybrid-minmax-pipeline", 
                           index_name: str = "documents", 
                           top_k: int = 10,
                           use_rerank: bool = True,
                           rerank_top_k: int = 3) -> List[Dict[str, Any]]:
        """
        Search pipelineì„ ì‚¬ìš©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        
        Args:
            query_text: ê²€ìƒ‰ ì¿¼ë¦¬ í…ìŠ¤íŠ¸
            keywords: í‚¤ì›Œë“œ ë˜ëŠ” í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ query_text ì‚¬ìš©)
            pipeline_id: ì‚¬ìš©í•  search pipeline ID
            index_name: ê²€ìƒ‰ ëŒ€ìƒ ì¸ë±ìŠ¤
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            use_rerank: ë¦¬ë­ì»¤ ì‚¬ìš© ì—¬ë¶€
            rerank_top_k: ë¦¬ë­í¬ í›„ ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ë¦¬ë­í¬ ì ìš© ì‹œ ë¦¬ë­í¬ëœ ê²°ê³¼)
        """
        if not self._check_client():
            return []
            
        logger.info(f"\n=== Search Pipeline ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘ ===")
        logger.info(f"Pipeline ID: {pipeline_id}")
        logger.info(f"Query: {query_text}")
        logger.info(f"Keywords: {keywords}")
        
        try:
            # í‚¤ì›Œë“œ ì²˜ë¦¬
            if keywords is None:
                keyword_text = query_text
            elif isinstance(keywords, list):
                keyword_text = " ".join(keywords)
            else:
                keyword_text = keywords
            
            # ë²¡í„° ì„ë² ë”© ìƒì„±
            query_vector = self.model.encode(query_text).tolist()
            logger.info(f"ìƒì„±ëœ ë²¡í„° ì°¨ì›: {len(query_vector)}")
            
            # í•˜ì´ë¸Œë¦¬ë“œ ì¿¼ë¦¬ êµ¬ì„±
            query_body = {
                "size": top_k,
                "query": {
                    "hybrid": {
                        "queries": [
                            {
                                "multi_match": {
                                    "query": keyword_text,
                                    "fields": ["content^2", "title^1.5", "file_name^1.0"],
                                    "type": "best_fields",
                                    "fuzziness": "AUTO"
                                }
                            },
                            {
                                "knn": {
                                    "embedding": {
                                        "vector": query_vector,
                                        "k": top_k
                                    }
                                }
                            }
                        ]
                    }
                },
                "_source": {
                    "excludes": ["embedding"]  # ë²¡í„° í•„ë“œ ì œì™¸
                }
            }

            # Search pipeline íŒŒë¼ë¯¸í„° ì„¤ì •
            params = {"search_pipeline": pipeline_id}
            
            logger.debug(f"ì‹¤í–‰ ì¤‘ì¸ ì¿¼ë¦¬: {json.dumps(query_body, indent=2, ensure_ascii=False)}")

            # ê²€ìƒ‰ ì‹¤í–‰
            response = self.client.search(index=index_name, body=query_body, params=params)
            
            # ê²°ê³¼ ì²˜ë¦¬
            hits = response.get("hits", {}).get("hits", [])
            results = []
            
            logger.info(f"âœ… Search pipeline ê²€ìƒ‰ ì™„ë£Œ: {len(hits)}ê°œ ê²°ê³¼")
            
            for i, hit in enumerate(hits):
                result = {
                    "score": hit["_score"],
                    "source": hit["_source"]
                }
                results.append(result)
                
                # ê²°ê³¼ ë¡œê¹…
                source = hit["_source"]
                logger.info(f"\n{i+1}. Pipeline ì ìˆ˜: {hit['_score']:.6f}")
                logger.info(f"   ë¬¸ì„œëª…: {source.get('title', 'N/A')}")
                logger.info(f"   íŒŒì¼ëª…: {source.get('file_name', 'N/A')}")
                logger.info(f"   ë‚´ìš©: {source.get('content', 'N/A')[:100]}...")
            
            # ë¦¬ë­í¬ ì ìš© (ë¬´ì¡°ê±´ ì ìš©)
            if results:
                if self.reranker:
                    logger.info(f"\nğŸ”„ BGE Rerankerë¡œ ìƒìœ„ {rerank_top_k}ê°œ ì„ ë³„ ì¤‘...")
                    reranked_results = self._rerank_documents_with_pipeline(query_text, results, rerank_top_k)
                    
                    logger.info(f"\n=== BGE Reranker ìµœì¢… ê²°ê³¼ (ìƒìœ„ {len(reranked_results)}ê°œ) ===")
                    for i, doc in enumerate(reranked_results):
                        source = doc['source']
                        logger.info(f"\n{i+1}. ë¦¬ë­í¬ ì ìˆ˜: {doc['rerank_score']:.6f}")
                        logger.info(f"   Pipeline ì ìˆ˜: {doc['score']:.6f}")
                        logger.info(f"   ë¬¸ì„œëª…: {source.get('title', 'N/A')}")
                        logger.info(f"   íŒŒì¼ëª…: {source.get('file_name', 'N/A')}")
                        logger.info(f"   ë‚´ìš©: {source.get('content', 'N/A')[:100]}...")
                    
                    return reranked_results
                else:
                    logger.warning("âš ï¸ ì¬ìˆœìœ„ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
                    return results
            else:
                logger.warning("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ ì¬ìˆœìœ„ë¥¼ ì ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return results
            
        except Exception as e:
            logger.error(f"âŒ Search pipeline ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _rerank_documents_with_pipeline(self, query_text: str, documents: List[Dict[str, Any]], top_k: int = 3) -> List[Dict[str, Any]]:
        """
        BGE Rerankerë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë¦¬ë­í¬ (Search Pipelineìš©)
        
        Args:
            query_text: ì¿¼ë¦¬ í…ìŠ¤íŠ¸
            documents: ë¦¬ë­í¬í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            top_k: ë°˜í™˜í•  ìƒìœ„ ë¬¸ì„œ ìˆ˜
            
        Returns:
            ë¦¬ë­í¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not documents or not self.reranker:
            return documents[:top_k]
        
        logger.info(f"BGE Reranker ë¦¬ë­í¬ ì‹œì‘ - ëŒ€ìƒ ë¬¸ì„œ: {len(documents)}ê°œ")
        
        try:
            # Rerankerë¥¼ ìœ„í•œ ì¿¼ë¦¬-ë¬¸ì„œ ìŒ ìƒì„±
            query_doc_pairs = []
            for doc in documents:
                source = doc['source']
                doc_text = f"{source.get('title', '')} {source.get('content', '')}"
                query_doc_pairs.append([query_text, doc_text])
            
            # Reranker ì ìˆ˜ ê³„ì‚°
            rerank_scores = self.reranker.compute_score(query_doc_pairs)
            
            if rerank_scores is None:
                logger.warning("ë¦¬ë­í¬ ì ìˆ˜ ê³„ì‚°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return documents[:top_k]
            
            # numpy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            if hasattr(rerank_scores, 'tolist'):
                rerank_scores = rerank_scores.tolist()
            else:
                rerank_scores = list(rerank_scores)
            
            logger.info(f"ë¦¬ë­í¬ ì ìˆ˜ ë²”ìœ„: {min(rerank_scores):.6f} ~ {max(rerank_scores):.6f}")
            
            # ì ìˆ˜ë¥¼ ë¬¸ì„œì— ì¶”ê°€
            for i, doc in enumerate(documents):
                doc['rerank_score'] = rerank_scores[i]
            
            # ë¦¬ë­í¬ ì ìˆ˜ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ top_kê°œ ë°˜í™˜
            reranked_results = sorted(documents, key=lambda x: x['rerank_score'], reverse=True)[:top_k]
            
            return reranked_results
            
        except Exception as e:
            logger.error(f"Reranker ì ìš© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return documents[:top_k]

# ì „ì—­ OpenSearch í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (10ì´ˆ íƒ€ì„ì•„ì›ƒ + 3íšŒ ì¬ì‹œë„)
opensearch_client = OpenSearchClient(max_retries=3, timeout=10) 